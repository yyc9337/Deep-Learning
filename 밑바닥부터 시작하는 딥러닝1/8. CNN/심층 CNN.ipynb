{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2138,
     "status": "ok",
     "timestamp": 1648000376654,
     "user": {
      "displayName": "영찬",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08284490542508903619"
     },
     "user_tz": -540
    },
    "id": "fZEDa9GZdTaw"
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(\"../예제/\")  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"정확도 99% 이상의 고정밀 합성곱 신경망\n",
    "\n",
    "    네트워크 구성은 아래와 같음\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 가중치 초기화===========\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1458477,
     "status": "ok",
     "timestamp": 1648002949112,
     "user": {
      "displayName": "영찬",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08284490542508903619"
     },
     "user_tz": -540
    },
    "id": "6qY2I4gLekZv",
    "outputId": "8372104a-d111-4155-e2c8-39ed7526a8c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.1235640943142768\n",
      "train loss:1.100872811718232\n",
      "train loss:1.1814605904942161\n",
      "train loss:1.1402861134152809\n",
      "train loss:1.0562544134298006\n",
      "train loss:1.1754423793038875\n",
      "train loss:0.9374889879753767\n",
      "train loss:1.020640506357812\n",
      "train loss:1.1969074211140098\n",
      "train loss:0.9811670601080901\n",
      "train loss:1.112242648348503\n",
      "train loss:1.1931456304010342\n",
      "train loss:0.9809828855743327\n",
      "train loss:1.1236295981633617\n",
      "train loss:1.1211928706627308\n",
      "train loss:0.9453743625088703\n",
      "train loss:0.9950891544645554\n",
      "train loss:1.0278962132824685\n",
      "train loss:1.1750957140831624\n",
      "train loss:1.0683426102323985\n",
      "train loss:1.0714715581195449\n",
      "train loss:1.006906679058985\n",
      "train loss:1.2483411963467566\n",
      "train loss:0.9862995415567829\n",
      "train loss:1.1675815508457992\n",
      "train loss:1.0622930931885208\n",
      "train loss:0.8967355913002939\n",
      "train loss:0.9779755960091802\n",
      "train loss:1.0601438095144593\n",
      "train loss:1.1383299420121886\n",
      "train loss:1.0505371458957447\n",
      "train loss:1.1237938312659852\n",
      "train loss:1.120608996037255\n",
      "train loss:0.9981147795712079\n",
      "train loss:1.0609881384613626\n",
      "train loss:1.1298487539814588\n",
      "train loss:0.7998460289237174\n",
      "train loss:1.0123120574282007\n",
      "train loss:0.9012730801541312\n",
      "train loss:1.2063236272058655\n",
      "train loss:0.96289732062032\n",
      "train loss:0.9164360896032374\n",
      "train loss:0.9263664991927618\n",
      "train loss:1.0014329670557796\n",
      "train loss:0.9365533769803259\n",
      "train loss:1.0265463747970254\n",
      "train loss:1.2613533966810841\n",
      "train loss:0.877273711045125\n",
      "train loss:1.1137237584452857\n",
      "train loss:1.1140308925698672\n",
      "train loss:1.0321459038957606\n",
      "train loss:1.0040557195614535\n",
      "train loss:1.1181331727909263\n",
      "train loss:1.0942898035394666\n",
      "train loss:1.2441691650813111\n",
      "train loss:1.1593240338794322\n",
      "train loss:1.0019491889830041\n",
      "train loss:1.0338759287638752\n",
      "train loss:1.1825912236895317\n",
      "train loss:1.098382916022971\n",
      "train loss:1.0623873264892176\n",
      "train loss:1.1508304072915287\n",
      "train loss:1.00819860525743\n",
      "train loss:1.0215763330451504\n",
      "train loss:0.9070011783730422\n",
      "train loss:1.0128856787296625\n",
      "train loss:1.0107839768254392\n",
      "train loss:1.1694738127477649\n",
      "train loss:0.925749034709022\n",
      "train loss:1.1573906136724714\n",
      "train loss:1.0236089686007752\n",
      "train loss:1.1418647956917207\n",
      "train loss:0.8654426826320649\n",
      "train loss:1.0295157003900166\n",
      "train loss:1.0309935685537308\n",
      "train loss:0.9447561674333794\n",
      "train loss:1.099955212433697\n",
      "train loss:1.1602950625164845\n",
      "train loss:0.8245675182267767\n",
      "train loss:0.877180270558926\n",
      "train loss:1.016248195820987\n",
      "train loss:1.2598541873486648\n",
      "train loss:1.1150608893225797\n",
      "train loss:1.1538385983635353\n",
      "train loss:1.017785804397225\n",
      "train loss:1.0016431554651455\n",
      "train loss:1.1614890148410328\n",
      "train loss:1.1803830936235113\n",
      "train loss:0.8570363653013452\n",
      "train loss:1.0611328582039286\n",
      "train loss:1.1931363858115658\n",
      "train loss:1.0934775074890835\n",
      "train loss:1.044699050714048\n",
      "train loss:1.0073370781066735\n",
      "train loss:0.7884543982276669\n",
      "train loss:1.0768119969868606\n",
      "train loss:1.0434181612493354\n",
      "train loss:0.9975013675557379\n",
      "train loss:0.9541588481989765\n",
      "train loss:1.070004522189548\n",
      "train loss:1.220128843278816\n",
      "train loss:0.976171032797238\n",
      "train loss:0.9478742458247587\n",
      "train loss:1.0727754815642545\n",
      "train loss:1.099597680670798\n",
      "train loss:1.14467747656331\n",
      "train loss:0.9720749080042073\n",
      "train loss:1.0270186802107415\n",
      "train loss:1.1296129547592715\n",
      "train loss:1.113691024339595\n",
      "train loss:1.1304760795663507\n",
      "train loss:0.886147769702299\n",
      "train loss:1.1403062014180427\n",
      "train loss:0.9172231512009777\n",
      "train loss:0.9963395624434959\n",
      "train loss:0.9617633443522702\n",
      "train loss:0.9277140553665585\n",
      "train loss:1.0796562215374375\n",
      "train loss:0.9141268577596694\n",
      "train loss:1.1399534603361547\n",
      "train loss:0.9353037659814045\n",
      "train loss:1.033436627927632\n",
      "train loss:1.088074276280856\n",
      "train loss:1.1312631113438774\n",
      "train loss:0.8421850317927987\n",
      "train loss:1.0852918728340644\n",
      "train loss:0.8586837538604049\n",
      "train loss:1.0387510050498536\n",
      "train loss:1.1290133706665404\n",
      "train loss:1.1065843016845378\n",
      "train loss:1.0275331024380414\n",
      "train loss:0.9413946189656932\n",
      "train loss:0.921735606402382\n",
      "train loss:1.0904985030174152\n",
      "train loss:0.9286070851643217\n",
      "train loss:1.2594495743975405\n",
      "train loss:0.9663461372065018\n",
      "train loss:0.8913982267237144\n",
      "train loss:0.8124365330360669\n",
      "train loss:1.0713772018863275\n",
      "train loss:1.0167125103364998\n",
      "train loss:0.8982937918081875\n",
      "train loss:1.1104037642858313\n",
      "train loss:0.8272428527555051\n",
      "train loss:0.9915800329865659\n",
      "train loss:0.7844704698936623\n",
      "train loss:0.9776583628845422\n",
      "train loss:1.0867546688020728\n",
      "train loss:1.116561941231879\n",
      "train loss:1.1008243604850578\n",
      "train loss:1.1029638375166007\n",
      "train loss:0.9968735541770475\n",
      "train loss:1.0203921533403078\n",
      "train loss:0.9728892971487404\n",
      "train loss:1.0181672294468094\n",
      "train loss:0.976714367247039\n",
      "train loss:1.141556344514968\n",
      "train loss:1.0599903442614411\n",
      "train loss:0.9737446599163805\n",
      "train loss:0.9759648059014815\n",
      "train loss:1.1185099509065666\n",
      "train loss:0.8381521466733601\n",
      "train loss:1.0207308118524914\n",
      "train loss:1.1057525350648154\n",
      "train loss:1.0869922977106505\n",
      "train loss:0.987824552891947\n",
      "train loss:0.9981652855378494\n",
      "train loss:1.168451675066295\n",
      "train loss:1.0624011187158926\n",
      "train loss:0.950861926325336\n",
      "train loss:0.826576631581747\n",
      "train loss:0.9090781289777202\n",
      "train loss:1.0058198735304993\n",
      "train loss:1.0830221879464097\n",
      "train loss:1.178686224873725\n",
      "train loss:1.0490247312592604\n",
      "train loss:1.103475125862804\n",
      "train loss:1.0091001272774807\n",
      "train loss:0.9812176750334999\n",
      "train loss:1.1411356004196793\n",
      "train loss:0.9732025727320095\n",
      "train loss:1.0076322634878714\n",
      "train loss:0.9954175045443675\n",
      "train loss:1.1452463776634552\n",
      "train loss:0.9941138139169253\n",
      "train loss:0.888629079054053\n",
      "train loss:1.0176051953443537\n",
      "train loss:1.2078696464639536\n",
      "train loss:1.0200925947916593\n",
      "train loss:1.022466618479726\n",
      "train loss:0.9780570877521215\n",
      "train loss:0.9979705784569307\n",
      "train loss:1.052443151808997\n",
      "train loss:0.871720956583106\n",
      "train loss:1.183633889009961\n",
      "train loss:0.9622416453586011\n",
      "train loss:1.0101808446435157\n",
      "train loss:0.9553846584889247\n",
      "train loss:1.1493758179462614\n",
      "train loss:1.076421587600717\n",
      "train loss:0.9825099905915614\n",
      "train loss:0.9759139593836621\n",
      "train loss:1.0591044632297988\n",
      "train loss:0.9537089483863052\n",
      "train loss:0.8627500849597161\n",
      "train loss:0.9362209279526849\n",
      "train loss:1.1621428588274947\n",
      "train loss:0.9628520802794196\n",
      "train loss:0.8410488993523486\n",
      "train loss:1.0155076581312\n",
      "train loss:1.1807955165331903\n",
      "train loss:0.9959676074605661\n",
      "train loss:1.0714893041984088\n",
      "train loss:0.8695522003935803\n",
      "train loss:0.9350689956892058\n",
      "train loss:1.1634582305995071\n",
      "train loss:1.089628147937037\n",
      "train loss:0.8815954609520918\n",
      "train loss:0.9512393108670445\n",
      "train loss:1.1146932082777423\n",
      "train loss:0.9166585408588039\n",
      "train loss:0.9421439289875373\n",
      "train loss:0.9118234120172432\n",
      "train loss:1.0046414765914689\n",
      "train loss:0.9414704178549101\n",
      "train loss:1.1546218439279516\n",
      "train loss:1.0438135397291348\n",
      "train loss:1.047045280426983\n",
      "train loss:1.1163913578381814\n",
      "train loss:0.9525849540792077\n",
      "train loss:1.0351902306428118\n",
      "train loss:1.1906389926846683\n",
      "train loss:1.114087745353008\n",
      "train loss:1.072508065774971\n",
      "train loss:0.8776522821826401\n",
      "train loss:1.01575626880949\n",
      "train loss:0.9471414843839909\n",
      "train loss:1.0103627070225405\n",
      "train loss:0.9271189996436843\n",
      "train loss:0.9782069040050864\n",
      "train loss:1.1778055178464528\n",
      "train loss:0.96414711142193\n",
      "train loss:1.1958937778204617\n",
      "train loss:0.9628947898043667\n",
      "train loss:0.7329144980412674\n",
      "train loss:0.979642034838978\n",
      "train loss:1.0162675287695389\n",
      "train loss:0.8965630978936755\n",
      "train loss:1.0974604511435735\n",
      "train loss:1.0361362625581838\n",
      "train loss:1.0508036728301124\n",
      "train loss:0.9711218637885173\n",
      "train loss:1.0664306982307878\n",
      "train loss:1.0329164041492829\n",
      "train loss:1.0153140113236487\n",
      "train loss:0.9957725285155306\n",
      "train loss:0.9883858005073024\n",
      "train loss:0.9856148225700457\n",
      "train loss:0.9516775194060655\n",
      "train loss:0.8489505735702708\n",
      "train loss:1.0543132550586842\n",
      "train loss:0.8988485051556283\n",
      "train loss:1.0224400951329355\n",
      "train loss:0.9932823457238588\n",
      "train loss:0.8201311736290934\n",
      "train loss:0.975137177395877\n",
      "train loss:0.9429500111828404\n",
      "train loss:1.1439687436776202\n",
      "train loss:1.0956450470728205\n",
      "train loss:1.049183891221547\n",
      "train loss:1.0146938649522785\n",
      "train loss:1.0762975090818043\n",
      "train loss:1.0858021612334618\n",
      "train loss:1.0754769220300804\n",
      "train loss:1.0302712357655563\n",
      "train loss:0.8872159271746513\n",
      "train loss:0.9997916654481943\n",
      "train loss:1.0596210963998085\n",
      "train loss:1.006179120290309\n",
      "train loss:1.1650321459671327\n",
      "train loss:1.067729173329272\n",
      "train loss:1.0555422254941778\n",
      "train loss:0.819071723254485\n",
      "train loss:0.8919449528133832\n",
      "train loss:1.073687364157803\n",
      "train loss:0.9798554541427579\n",
      "train loss:1.0540247078456622\n",
      "train loss:0.915678910317412\n",
      "train loss:1.0591184906215494\n",
      "train loss:0.9223287042268972\n",
      "train loss:0.9511132361245751\n",
      "train loss:1.0009179552477259\n",
      "train loss:1.1528584051626656\n",
      "train loss:1.1392472319973743\n",
      "train loss:1.0505173899730404\n",
      "train loss:1.1539333942092294\n",
      "train loss:1.0956261094173003\n",
      "train loss:1.127400885908365\n",
      "train loss:1.0852789211235416\n",
      "train loss:0.9512163920302277\n",
      "train loss:1.0222753395534823\n",
      "train loss:1.0074672193168528\n",
      "train loss:1.0163107465645447\n",
      "train loss:0.9536964479365051\n",
      "train loss:0.8770125236012877\n",
      "train loss:1.131152189345742\n",
      "train loss:1.0461018728046976\n",
      "train loss:0.9753670412392448\n",
      "train loss:1.0642717689802297\n",
      "train loss:1.1014043666915796\n",
      "train loss:0.9574001464154389\n",
      "train loss:0.9332204433754994\n",
      "train loss:0.9137698726236946\n",
      "train loss:1.054423221684208\n",
      "train loss:1.1138945486424703\n",
      "train loss:1.248304192688493\n",
      "train loss:1.0656157049922228\n",
      "train loss:0.8210660024044144\n",
      "train loss:1.086153598979871\n",
      "train loss:1.0981883285418481\n",
      "train loss:0.8300723517404681\n",
      "train loss:1.1752301767685298\n",
      "train loss:0.9989359301416418\n",
      "train loss:1.1777234379601447\n",
      "train loss:1.0417252648997808\n",
      "train loss:1.0312887997072986\n",
      "train loss:0.7966229711565052\n",
      "train loss:0.9378514039880423\n",
      "train loss:0.9850496942777704\n",
      "train loss:1.1106996983489943\n",
      "train loss:1.3111983056931142\n",
      "train loss:1.0302425781798876\n",
      "train loss:1.0799896249841339\n",
      "train loss:0.8725423542557829\n",
      "train loss:1.0671261482345975\n",
      "train loss:0.9768941950388123\n",
      "train loss:1.0541136696244962\n",
      "train loss:0.9745638853201684\n",
      "train loss:1.091000218769404\n",
      "train loss:0.7871471107571023\n",
      "train loss:1.0528024978279293\n",
      "train loss:1.0001046421793707\n",
      "train loss:1.2445008895075997\n",
      "train loss:0.9151119922210488\n",
      "train loss:1.0442863068804258\n",
      "train loss:1.087908302658621\n",
      "train loss:0.9708580836916313\n",
      "train loss:1.0455143510270302\n",
      "train loss:1.079648880889406\n",
      "train loss:1.0109752449654617\n",
      "train loss:1.090007512694507\n",
      "train loss:1.0754445288324301\n",
      "train loss:1.1155744856250724\n",
      "train loss:0.9778079310658814\n",
      "train loss:1.0451132444700337\n",
      "train loss:1.0270107784420608\n",
      "train loss:1.048592751503806\n",
      "train loss:1.0987280471247551\n",
      "train loss:1.1051659048243716\n",
      "train loss:1.0511453014643641\n",
      "train loss:1.0527393221236192\n",
      "train loss:1.0369183865933096\n",
      "train loss:0.9844864215557336\n",
      "train loss:0.9562990995761489\n",
      "train loss:0.9378719315484836\n",
      "train loss:1.1765538235392419\n",
      "train loss:0.8938274883974825\n",
      "train loss:1.0987709176943083\n",
      "train loss:0.9584191835373835\n",
      "train loss:1.139397399818196\n",
      "train loss:1.0176407052195275\n",
      "train loss:1.0978806829773828\n",
      "train loss:1.0141219805548993\n",
      "train loss:1.1563464874304115\n",
      "train loss:1.084667880158812\n",
      "train loss:1.0149260852865403\n",
      "train loss:0.905161647545219\n",
      "train loss:0.9627407364486632\n",
      "train loss:0.9578748833217371\n",
      "train loss:1.0743652187436417\n",
      "train loss:1.0716106675158452\n",
      "train loss:0.8894467648609868\n",
      "train loss:1.035127930394375\n",
      "train loss:1.0092184056823155\n",
      "train loss:1.0388714327873763\n",
      "train loss:1.172261340490447\n",
      "train loss:1.118289106273775\n",
      "train loss:0.9018742836053412\n",
      "train loss:0.9166269166980916\n",
      "train loss:0.9163445028103377\n",
      "train loss:0.9084854050400808\n",
      "train loss:0.8429147482801899\n",
      "train loss:1.069604807526926\n",
      "train loss:1.263912737104473\n",
      "train loss:0.9653981768943034\n",
      "train loss:1.1077631181242555\n",
      "train loss:1.244066867845086\n",
      "train loss:0.8862033806414721\n",
      "train loss:0.9950534669614848\n",
      "train loss:0.9753887321444407\n",
      "train loss:1.0566887507427845\n",
      "train loss:1.2454178297518523\n",
      "train loss:1.0990534683605875\n",
      "train loss:0.9903476230566358\n",
      "train loss:1.1359895351894838\n",
      "train loss:1.0055301634455511\n",
      "train loss:0.9690730639831822\n",
      "train loss:0.9950399732933073\n",
      "train loss:1.0025619574598525\n",
      "train loss:1.1923769066278498\n",
      "train loss:1.0427096398792335\n",
      "train loss:0.9430981740528901\n",
      "train loss:1.0474498699741344\n",
      "train loss:1.0527807265779225\n",
      "train loss:1.1702175071266603\n",
      "train loss:1.0464454451541043\n",
      "train loss:1.0694738049395285\n",
      "train loss:1.1457470937188143\n",
      "train loss:0.8310747235985667\n",
      "train loss:1.0751240953353207\n",
      "train loss:0.9693991316677419\n",
      "train loss:0.9032223393303574\n",
      "train loss:1.2325427865432668\n",
      "train loss:0.97612014688036\n",
      "train loss:0.9986475021994401\n",
      "train loss:0.7884525326386786\n",
      "train loss:1.0034068334054993\n",
      "train loss:0.9814682711375068\n",
      "train loss:0.9945278931981538\n",
      "train loss:1.0920619895094332\n",
      "train loss:0.9566194958849331\n",
      "train loss:0.9958448477947337\n",
      "train loss:1.1397880513158951\n",
      "train loss:1.079670483336225\n",
      "train loss:1.0226210059183463\n",
      "train loss:0.8819379772337841\n",
      "train loss:0.8236211053754667\n",
      "train loss:1.1414488523394128\n",
      "train loss:0.9257736138704037\n",
      "train loss:1.1252922042252362\n",
      "train loss:0.9872007567224281\n",
      "train loss:1.1791732019083736\n",
      "train loss:0.9329967147358263\n",
      "train loss:1.0099026835281055\n",
      "train loss:1.1005959296909993\n",
      "train loss:0.9855931165464625\n",
      "train loss:0.9250059265168885\n",
      "train loss:1.096794986953835\n",
      "train loss:0.9068171408861407\n",
      "train loss:0.9239826715295546\n",
      "train loss:0.8463832642216338\n",
      "train loss:0.8292727623594816\n",
      "train loss:1.0461619353633604\n",
      "train loss:1.0495958150111457\n",
      "train loss:0.9551147564085352\n",
      "train loss:0.9376742616389123\n",
      "train loss:1.000577526571446\n",
      "train loss:0.9531530925169157\n",
      "train loss:0.8453997797970686\n",
      "train loss:0.9113503847532943\n",
      "train loss:0.9118812896969114\n",
      "train loss:0.907184666530634\n",
      "train loss:0.9326292783548713\n",
      "train loss:0.922977020924841\n",
      "train loss:0.982252886720581\n",
      "train loss:0.813238164939012\n",
      "train loss:0.8731972486225223\n",
      "train loss:1.0550088643970676\n",
      "train loss:0.9462739477581116\n",
      "train loss:0.956895490672525\n",
      "train loss:1.1454319018088108\n",
      "train loss:1.1836410218235924\n",
      "train loss:1.0842238671537368\n",
      "train loss:1.023951390034084\n",
      "train loss:1.0225060152324672\n",
      "train loss:1.104785274708956\n",
      "train loss:0.8851031442554043\n",
      "train loss:1.0078695973951597\n",
      "=== epoch:3, train acc:0.98, test acc:0.977 ===\n",
      "train loss:1.127053957737369\n",
      "train loss:0.9114959681743292\n",
      "train loss:0.8600173677516344\n",
      "train loss:1.0097298839145252\n",
      "train loss:1.1063030561711455\n",
      "train loss:0.8338774915931699\n",
      "train loss:1.0420406247115317\n",
      "train loss:0.9902689797588319\n",
      "train loss:1.057378357484377\n",
      "train loss:0.8751500110171729\n",
      "train loss:1.1089362587211793\n",
      "train loss:1.1712826185715746\n",
      "train loss:0.9418253859878518\n",
      "train loss:0.8648292810700048\n",
      "train loss:0.9387551232314872\n",
      "train loss:0.9072892124170581\n",
      "train loss:1.0126620872031822\n",
      "train loss:1.048617539994885\n",
      "train loss:0.953235684706075\n",
      "train loss:1.0983857527740966\n",
      "train loss:0.8172165780430705\n",
      "train loss:0.8825741402797067\n",
      "train loss:0.9616774153821366\n",
      "train loss:0.9270168577332797\n",
      "train loss:1.0999178357535242\n",
      "train loss:1.0901140145828752\n",
      "train loss:1.0683438429992884\n",
      "train loss:1.0548736013111537\n",
      "train loss:0.907382048125748\n",
      "train loss:1.1266651316645555\n",
      "train loss:1.0430348291302798\n",
      "train loss:0.9933743467832984\n",
      "train loss:0.8699874239666024\n",
      "train loss:0.8975514861830715\n",
      "train loss:1.1281915065833013\n",
      "train loss:0.7355853313658407\n",
      "train loss:1.0766650371153197\n",
      "train loss:1.1839391034162103\n",
      "train loss:1.0491044338952171\n",
      "train loss:0.6988939181550733\n",
      "train loss:0.9503832981792576\n",
      "train loss:0.9257287242587989\n",
      "train loss:0.941877530453469\n",
      "train loss:0.9016472989306966\n",
      "train loss:0.9749511761547079\n",
      "train loss:1.0457398800543602\n",
      "train loss:0.9872793455118632\n",
      "train loss:0.8316563976914074\n",
      "train loss:1.1325927910278073\n",
      "train loss:0.9440536370241159\n",
      "train loss:0.9808343656419762\n",
      "train loss:0.8590435694851523\n",
      "train loss:1.233526125975241\n",
      "train loss:1.1712827115885254\n",
      "train loss:0.8618532006567498\n",
      "train loss:0.9726279980463493\n",
      "train loss:1.0222808072389602\n",
      "train loss:0.8883085927633193\n",
      "train loss:1.0087077528393877\n",
      "train loss:1.1419275858891977\n",
      "train loss:0.9239107465483386\n",
      "train loss:1.0593267736224932\n",
      "train loss:0.9278082723879783\n",
      "train loss:0.8974414431295987\n",
      "train loss:1.0592030792419753\n",
      "train loss:0.9250431905794609\n",
      "train loss:0.8829879464500197\n",
      "train loss:0.9224476271184036\n",
      "train loss:0.9791331025834829\n",
      "train loss:0.9812833107850796\n",
      "train loss:0.8721272598017213\n",
      "train loss:1.046162612103753\n",
      "train loss:0.9964367839993752\n",
      "train loss:1.0291411816007345\n",
      "train loss:1.0037643171554438\n",
      "train loss:0.9233629419204544\n",
      "train loss:0.9093639038240284\n",
      "train loss:0.8150383675426994\n",
      "train loss:0.9391278189609084\n",
      "train loss:0.9794185177405192\n",
      "train loss:1.0432452063737798\n",
      "train loss:0.9386667776820636\n",
      "train loss:1.0435527779861267\n",
      "train loss:0.9657040407325337\n",
      "train loss:1.0494833058808493\n",
      "train loss:0.9737310592211713\n",
      "train loss:0.9362286460883367\n",
      "train loss:1.064464145980545\n",
      "train loss:0.9477545058883018\n",
      "train loss:0.9058332381880574\n",
      "train loss:0.9545931410043924\n",
      "train loss:1.1971138427615324\n",
      "train loss:0.9442740211189473\n",
      "train loss:1.0063837361905204\n",
      "train loss:0.9418651195389016\n",
      "train loss:0.9246206942810631\n",
      "train loss:0.9922689787303439\n",
      "train loss:1.045582641943729\n",
      "train loss:0.8322256243509526\n",
      "train loss:0.9810678883378745\n",
      "train loss:1.0374960221064842\n",
      "train loss:1.0082565421403782\n",
      "train loss:0.9704091838075487\n",
      "train loss:0.9322406245953689\n",
      "train loss:1.153675543963982\n",
      "train loss:1.1223673133840137\n",
      "train loss:1.0376917588727697\n",
      "train loss:0.9705639777378086\n",
      "train loss:0.8832503922166292\n",
      "train loss:1.2836611367634543\n",
      "train loss:0.9761083225755114\n",
      "train loss:0.9571930546798417\n",
      "train loss:0.9004898009526907\n",
      "train loss:0.9174585548438459\n",
      "train loss:1.0375641094140833\n",
      "train loss:0.9522122145458828\n",
      "train loss:0.8444391604427334\n",
      "train loss:0.9233339556678314\n",
      "train loss:0.8737126647028348\n",
      "train loss:0.9565818712311415\n",
      "train loss:1.0200012301708161\n",
      "train loss:1.1174902026487528\n",
      "train loss:1.0680036354263525\n",
      "train loss:0.9201209102719478\n",
      "train loss:0.9367327296490702\n",
      "train loss:1.0830365142084633\n",
      "train loss:1.04830348703156\n",
      "train loss:0.8500982074590078\n",
      "train loss:1.0288862889255832\n",
      "train loss:0.9100195978282393\n",
      "train loss:0.9999738283241648\n",
      "train loss:0.9403595175110049\n",
      "train loss:1.0173930095318315\n",
      "train loss:0.927403647226607\n",
      "train loss:1.0349673208606989\n",
      "train loss:0.9306068105430407\n",
      "train loss:1.0432592568714854\n",
      "train loss:0.7282597299778398\n",
      "train loss:0.9885486113860307\n",
      "train loss:1.0106050863728806\n",
      "train loss:1.0674511999267866\n",
      "train loss:0.941997265504357\n",
      "train loss:0.8835284018230807\n",
      "train loss:1.0137765263500342\n",
      "train loss:1.0645409755310409\n",
      "train loss:1.1585971945162574\n",
      "train loss:1.094241418833059\n",
      "train loss:0.9921075919506401\n",
      "train loss:1.1442736455734108\n",
      "train loss:0.9129295454617409\n",
      "train loss:0.927824669174539\n",
      "train loss:0.8828329085812736\n",
      "train loss:0.968245741355678\n",
      "train loss:1.1138050356232072\n",
      "train loss:0.9417541451909986\n",
      "train loss:1.0824722623506904\n",
      "train loss:1.0139667592876598\n",
      "train loss:1.010548274710069\n",
      "train loss:1.0303383844170446\n",
      "train loss:0.9368234446511262\n",
      "train loss:0.8468166227203731\n",
      "train loss:1.2026967087700653\n",
      "train loss:0.8473266971452417\n",
      "train loss:0.8192084004740998\n",
      "train loss:1.0265010980418279\n",
      "train loss:1.0535797758031995\n",
      "train loss:1.0542309662850413\n",
      "train loss:0.9533869946629739\n",
      "train loss:0.952815547946431\n",
      "train loss:1.0423511998209423\n",
      "train loss:1.0282502945634058\n",
      "train loss:1.0952300365261434\n",
      "train loss:0.9442906695029134\n",
      "train loss:0.9781838609940575\n",
      "train loss:1.132280307314133\n",
      "train loss:0.9856031575427247\n",
      "train loss:1.0789572277740438\n",
      "train loss:0.9434912071648703\n",
      "train loss:0.8980455204516941\n",
      "train loss:1.091559635666972\n",
      "train loss:0.8881848275198704\n",
      "train loss:1.1030976235233003\n",
      "train loss:1.0049034481079857\n",
      "train loss:0.9691951156292414\n",
      "train loss:1.0184748624389437\n",
      "train loss:1.2165298565674012\n",
      "train loss:0.9626396380817082\n",
      "train loss:0.9322917437248328\n",
      "train loss:1.1754948383208872\n",
      "train loss:0.9664838365786329\n",
      "train loss:1.0053927465215764\n",
      "train loss:0.9568512455984195\n",
      "train loss:0.9194528296281722\n",
      "train loss:0.8462126214201103\n",
      "train loss:1.083441017337845\n",
      "train loss:1.0188996423007715\n",
      "train loss:1.1072956026956822\n",
      "train loss:1.0469293885775608\n",
      "train loss:0.9694298546031284\n",
      "train loss:1.0922681139422656\n",
      "train loss:1.0036711327484835\n",
      "train loss:0.9572502140474909\n",
      "train loss:0.8802527606922061\n",
      "train loss:1.1888813319949703\n",
      "train loss:1.0471099044488368\n",
      "train loss:0.9083402274901181\n",
      "train loss:1.0336630377091596\n",
      "train loss:1.070136266227265\n",
      "train loss:1.0219007255536092\n",
      "train loss:0.9456660094071495\n",
      "train loss:0.9234595381542906\n",
      "train loss:1.0407582880092796\n",
      "train loss:1.0305831531060141\n",
      "train loss:0.9748808283499999\n",
      "train loss:1.0814872569812177\n",
      "train loss:0.8729812379648504\n",
      "train loss:1.0813016815494911\n",
      "train loss:0.9541436097436996\n",
      "train loss:0.9140098208278447\n",
      "train loss:0.9938825612861989\n",
      "train loss:1.239026535992017\n",
      "train loss:1.1529854289731691\n",
      "train loss:0.7984453787813847\n",
      "train loss:0.8586527277190296\n",
      "train loss:0.9709962553864162\n",
      "train loss:1.020498017328712\n",
      "train loss:1.0599721335852546\n",
      "train loss:1.0881712279741302\n",
      "train loss:0.9586228780052068\n",
      "train loss:0.9386149424500116\n",
      "train loss:0.9012963347042099\n",
      "train loss:1.1310388696895077\n",
      "train loss:1.0372387864974606\n",
      "train loss:0.9988786919863238\n",
      "train loss:0.9759995788708329\n",
      "train loss:1.1463046661616503\n",
      "train loss:0.9778878320601859\n",
      "train loss:1.0836799347425095\n",
      "train loss:0.8836433030562305\n",
      "train loss:0.8838580166053134\n",
      "train loss:1.123330589302608\n",
      "train loss:0.9864469970264073\n",
      "train loss:1.0452079839007178\n",
      "train loss:0.9465607499391249\n",
      "train loss:1.0076038565576437\n",
      "train loss:0.84864777507671\n",
      "train loss:0.7221390686534861\n",
      "train loss:0.974238819800493\n",
      "train loss:0.8935829389934589\n",
      "train loss:1.0805216713285093\n",
      "train loss:0.8780364623541446\n",
      "train loss:1.1277559578292284\n",
      "train loss:0.8404792254508006\n",
      "train loss:0.8452288109834336\n",
      "train loss:0.8232240350577862\n",
      "train loss:1.066585656511393\n",
      "train loss:0.8776070840257737\n",
      "train loss:0.954114245774745\n",
      "train loss:0.9864200747140132\n",
      "train loss:0.8336580677091816\n",
      "train loss:0.985991274770852\n",
      "train loss:1.0027763196037442\n",
      "train loss:0.9395427511597948\n",
      "train loss:1.059919978053415\n",
      "train loss:1.06554104060326\n",
      "train loss:0.9004392134272139\n",
      "train loss:0.9702696113198276\n",
      "train loss:0.9655424111738306\n",
      "train loss:0.9652079995036938\n",
      "train loss:1.105429400767937\n",
      "train loss:1.1363512167569454\n",
      "train loss:0.8510539607708424\n",
      "train loss:0.9605534389292052\n",
      "train loss:1.0457171226981823\n",
      "train loss:0.9136821375885933\n",
      "train loss:1.0709433585014723\n",
      "train loss:1.0603338200741783\n",
      "train loss:0.9374557621769993\n",
      "train loss:0.863557771864532\n",
      "train loss:0.9508598202274621\n",
      "train loss:0.9967032912358234\n",
      "train loss:1.0919681604821283\n",
      "train loss:1.0056446376142452\n",
      "train loss:1.0308729185691092\n",
      "train loss:0.9535614737495587\n",
      "train loss:0.9303364066068718\n",
      "train loss:1.0998905739899265\n",
      "train loss:0.8464353860026581\n",
      "train loss:1.126187585943238\n",
      "train loss:1.0635109311715436\n",
      "train loss:1.0538965444495028\n",
      "train loss:0.943704933331794\n",
      "train loss:1.0338109777105309\n",
      "train loss:0.9138631056727184\n",
      "train loss:0.9794793891848478\n",
      "train loss:0.9897431796519344\n",
      "train loss:0.7833212736759441\n",
      "train loss:1.1526970759940205\n",
      "train loss:0.9631712060681678\n",
      "train loss:0.9651213035500733\n",
      "train loss:1.0053893478863767\n",
      "train loss:0.7583359514880431\n",
      "train loss:1.0107434962227015\n",
      "train loss:0.8092499266260768\n",
      "train loss:0.986285097099534\n",
      "train loss:0.864570082307513\n",
      "train loss:1.0740034906517935\n",
      "train loss:0.8641077941493154\n",
      "train loss:1.0867251493358605\n",
      "train loss:1.007127318643716\n",
      "train loss:0.9319452982725438\n",
      "train loss:1.0590698079497274\n",
      "train loss:0.9381068910666444\n",
      "train loss:0.9828281354414043\n",
      "train loss:0.9344290339553715\n",
      "train loss:0.9779302393475592\n",
      "train loss:1.0030393738454677\n",
      "train loss:0.9498085866499066\n",
      "train loss:0.8786921521072497\n",
      "train loss:0.8241612804399506\n",
      "train loss:1.024734623547138\n",
      "train loss:0.9334867079897674\n",
      "train loss:0.9885620793791192\n",
      "train loss:0.8513180248282012\n",
      "train loss:1.0469058804911413\n",
      "train loss:0.8189105353264775\n",
      "train loss:1.0122141255853163\n",
      "train loss:1.0389985777386892\n",
      "train loss:1.0660803453593628\n",
      "train loss:0.8421901899990667\n",
      "train loss:0.932008579359184\n",
      "train loss:0.9337248691606601\n",
      "train loss:0.9736058321732081\n",
      "train loss:1.003992095078918\n",
      "train loss:0.9043747032395091\n",
      "train loss:0.9726719348181743\n",
      "train loss:1.0697015649782808\n",
      "train loss:0.7459434700643257\n",
      "train loss:0.8635122763080086\n",
      "train loss:0.8762324050199038\n",
      "train loss:0.7212479489036709\n",
      "train loss:0.8794661204905968\n",
      "train loss:0.8978814065289563\n",
      "train loss:1.2861293736856592\n",
      "train loss:0.8395155012840664\n",
      "train loss:0.7499069236587611\n",
      "train loss:1.3013976820550266\n",
      "train loss:1.0152479338111822\n",
      "train loss:1.144250914306064\n",
      "train loss:1.15851315064043\n",
      "train loss:0.822250725028907\n",
      "train loss:0.9778035268823427\n",
      "train loss:0.9162971182441096\n",
      "train loss:1.025296455136157\n",
      "train loss:0.730472748218791\n",
      "train loss:0.9749666408456906\n",
      "train loss:0.8734803261810957\n",
      "train loss:1.030320782269627\n",
      "train loss:0.949815130881004\n",
      "train loss:0.8839455667971898\n",
      "train loss:0.935857043780329\n",
      "train loss:0.8395700682937243\n",
      "train loss:0.9178220272085531\n",
      "train loss:1.2056157652398678\n",
      "train loss:0.8612492651283337\n",
      "train loss:0.8395644229110283\n",
      "train loss:0.8551158248729794\n",
      "train loss:0.7919598461653525\n",
      "train loss:0.8928905803173095\n",
      "train loss:0.9749032801287515\n",
      "train loss:1.0855504231877833\n",
      "train loss:1.1695393204424511\n",
      "train loss:0.9518024445314844\n",
      "train loss:0.9776631996633307\n",
      "train loss:1.1656830407831533\n",
      "train loss:1.0048272472183561\n",
      "train loss:0.9647824338321785\n",
      "train loss:1.0565863199142362\n",
      "train loss:0.7828448072453987\n",
      "train loss:1.1249146582966427\n",
      "train loss:0.9366994089515834\n",
      "train loss:0.9426385976264564\n",
      "train loss:0.932075089359331\n",
      "train loss:1.0766435089344248\n",
      "train loss:0.8558424868650182\n",
      "train loss:0.9553527978224844\n",
      "train loss:0.6518585865988962\n",
      "train loss:1.0401680568547056\n",
      "train loss:1.0108558939951744\n",
      "train loss:1.0435169534361792\n",
      "train loss:0.9528654112257828\n",
      "train loss:0.9918857681933817\n",
      "train loss:1.192001564409865\n",
      "train loss:1.064187605214649\n",
      "train loss:1.0870512001167252\n",
      "train loss:1.0091028643719973\n",
      "train loss:0.8350037695749067\n",
      "train loss:1.0660229292997232\n",
      "train loss:0.863595898843817\n",
      "train loss:1.0323591421609797\n",
      "train loss:0.8654164967223018\n",
      "train loss:1.0270886369991081\n",
      "train loss:1.0456168458621393\n",
      "train loss:0.793667639156883\n",
      "train loss:0.9048127809770162\n",
      "train loss:1.0811029132335666\n",
      "train loss:0.9582557847174122\n",
      "train loss:0.8596148837918013\n",
      "train loss:1.0298476294292565\n",
      "train loss:0.7845265999246461\n",
      "train loss:0.9516645419139167\n",
      "train loss:0.9554342797431559\n",
      "train loss:0.786240140436464\n",
      "train loss:0.7804392958385801\n",
      "train loss:0.9755024698379903\n",
      "train loss:1.1354946927425869\n",
      "train loss:0.954142367857288\n",
      "train loss:0.9546791562892613\n",
      "train loss:0.9798754748175124\n",
      "train loss:0.8769947738304572\n",
      "train loss:0.9213957924740289\n",
      "train loss:0.9567857940087177\n",
      "train loss:0.936341782600762\n",
      "train loss:0.9901125914022334\n",
      "train loss:1.0063320116209589\n",
      "train loss:1.110494587692919\n",
      "train loss:0.8615042752145964\n",
      "train loss:0.9067058485546613\n",
      "train loss:1.0090550212727165\n",
      "train loss:0.8268710058978793\n",
      "train loss:0.795356144585065\n",
      "train loss:1.0254731288063386\n",
      "train loss:0.9388598284384797\n",
      "train loss:0.979644061469411\n",
      "train loss:0.7755645102664759\n",
      "train loss:1.0554683617583662\n",
      "train loss:0.9803351871654177\n",
      "train loss:1.0772139819037019\n",
      "train loss:1.1613793203689913\n",
      "train loss:1.044396272780241\n",
      "train loss:0.8962927832638714\n",
      "train loss:1.0842451366542216\n",
      "train loss:1.0054604272955627\n",
      "train loss:0.92686351429669\n",
      "train loss:1.018722805242169\n",
      "train loss:0.8845610653658976\n",
      "train loss:0.957497994302109\n",
      "train loss:0.8993573009018803\n",
      "train loss:1.035531180924521\n",
      "train loss:1.172403659692167\n",
      "train loss:0.8597159349174845\n",
      "train loss:1.051819903015227\n",
      "train loss:1.097340699780904\n",
      "train loss:0.913800584425346\n",
      "train loss:0.8044240598525773\n",
      "train loss:1.072811062378289\n",
      "train loss:0.9155845073478968\n",
      "train loss:1.0165138111950134\n",
      "train loss:0.7903639546559443\n",
      "train loss:1.0236010976414636\n",
      "train loss:0.842573415318399\n",
      "train loss:0.987443276242372\n",
      "train loss:0.999466360134697\n",
      "train loss:0.6572954926559263\n",
      "train loss:0.9718827940986492\n",
      "train loss:0.9755770469634854\n",
      "train loss:0.9914555985230993\n",
      "train loss:0.9773050395719936\n",
      "train loss:0.8581756403304323\n",
      "train loss:1.0033228201858773\n",
      "train loss:0.8930702520965328\n",
      "train loss:0.9095208653581963\n",
      "train loss:1.085505616258032\n",
      "train loss:0.9445692505997989\n",
      "train loss:1.1319095728909128\n",
      "train loss:0.9644410220391968\n",
      "train loss:0.8566710438846245\n",
      "train loss:0.8754945562467334\n",
      "train loss:0.9223874763026951\n",
      "train loss:0.9389367086087872\n",
      "train loss:1.0547816678840227\n",
      "train loss:1.0320333847777206\n",
      "train loss:0.872147148336781\n",
      "train loss:1.1327215151942014\n",
      "train loss:1.0435489320612985\n",
      "train loss:1.0140303006384666\n",
      "train loss:0.9809415414052312\n",
      "train loss:0.9368093535670851\n",
      "train loss:1.0924089152448948\n",
      "train loss:0.9124747193383824\n",
      "train loss:1.0562862803196151\n",
      "train loss:1.0889936265971758\n",
      "train loss:0.9805263002064154\n",
      "train loss:0.9912421466422157\n",
      "train loss:0.9474600315881426\n",
      "train loss:1.1025548077404907\n",
      "train loss:0.8558902131458703\n",
      "train loss:0.946036839131528\n",
      "train loss:0.8829460651225213\n",
      "train loss:1.101337748907818\n",
      "train loss:1.0028088993684676\n",
      "train loss:1.2060903778215617\n",
      "train loss:0.86107787954111\n",
      "train loss:1.0299758644741983\n",
      "train loss:0.8330887308141114\n",
      "train loss:1.0765178508461841\n",
      "train loss:0.9111195826106995\n",
      "train loss:0.9710205949892016\n",
      "train loss:0.9946326240681265\n",
      "train loss:1.035483130158777\n",
      "train loss:1.0968387066388854\n",
      "train loss:0.8520241128498863\n",
      "train loss:1.001542217143787\n",
      "train loss:0.7221905776860861\n",
      "train loss:1.0127097777674263\n",
      "train loss:1.1759038908559105\n",
      "train loss:1.07300995848813\n",
      "train loss:0.8774468486609607\n",
      "train loss:1.079889113315495\n",
      "train loss:0.9145903118006504\n",
      "train loss:0.894098619477533\n",
      "train loss:0.8410148924911302\n",
      "train loss:1.122149335443744\n",
      "train loss:0.9820782065049479\n",
      "train loss:1.0808871802965705\n",
      "train loss:0.9608358115872663\n",
      "train loss:0.9505489918528234\n",
      "train loss:1.0375917683567744\n",
      "train loss:1.013352242171308\n",
      "train loss:0.9987616960882538\n",
      "train loss:1.065197696809677\n",
      "train loss:0.9641971330178742\n",
      "train loss:0.9293953189510593\n",
      "train loss:1.0102497041946423\n",
      "train loss:1.1570904933612864\n",
      "train loss:0.8629225533425259\n",
      "train loss:1.0501343766190816\n",
      "train loss:0.8988983008335693\n",
      "train loss:1.0886338129059947\n",
      "train loss:0.8550846766879556\n",
      "train loss:0.9498802426581188\n",
      "train loss:0.8589148815776206\n",
      "train loss:0.9608758822665382\n",
      "train loss:1.1030528223190248\n",
      "train loss:0.976687609990756\n",
      "train loss:0.9466460173008916\n",
      "train loss:0.9256078179349805\n",
      "train loss:0.7607449798920948\n",
      "train loss:0.8853176175696252\n",
      "train loss:1.0178463265397344\n",
      "train loss:1.0211168046688248\n",
      "train loss:1.060988561802811\n",
      "train loss:0.9859865292924087\n",
      "train loss:0.970729235606625\n",
      "train loss:0.8722332826015657\n",
      "train loss:0.8056169854471323\n",
      "train loss:0.9772686968069422\n",
      "train loss:0.9971154174590825\n",
      "train loss:0.9277915760869891\n",
      "train loss:0.9994901520581925\n",
      "train loss:0.9561272043407812\n",
      "train loss:0.951629139077388\n",
      "train loss:1.0084231077621875\n",
      "train loss:1.063413201842566\n",
      "train loss:0.9921953528116432\n",
      "train loss:0.9083273490086557\n",
      "train loss:0.9357957117035052\n",
      "train loss:1.1287849008785764\n",
      "train loss:0.9182371597392087\n",
      "train loss:1.0314104437561182\n",
      "train loss:0.9378655131365337\n",
      "train loss:0.8931055522507506\n",
      "train loss:0.8360591354237503\n",
      "train loss:0.9649940214480544\n",
      "train loss:1.035311648404393\n",
      "train loss:0.8531080613202425\n",
      "train loss:1.1288921345230696\n",
      "train loss:1.06637452413764\n",
      "train loss:0.8892632001528061\n",
      "train loss:0.8349174950242215\n",
      "train loss:0.9878368125268263\n",
      "train loss:0.9087966919486102\n",
      "train loss:1.021530840795098\n",
      "train loss:1.0264889044358692\n",
      "train loss:1.0052072510499253\n",
      "train loss:0.9177321706841356\n",
      "train loss:0.6908490051090855\n",
      "train loss:0.9594573510982292\n",
      "train loss:0.9290988497618904\n",
      "train loss:0.9826794748783663\n",
      "train loss:0.9117006968707198\n",
      "train loss:0.9868788915833018\n",
      "train loss:1.0793636031770388\n",
      "train loss:1.0304336719552345\n",
      "train loss:1.137261740103957\n",
      "train loss:0.8909961308934878\n",
      "train loss:1.0078831606487508\n",
      "train loss:1.054734257953524\n",
      "train loss:0.946183496828462\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9895\n",
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=3, mini_batch_size=100,  ## epoch은 시간이 너무 오래걸려서 3epohc만함\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보관\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1648003087683,
     "user": {
      "displayName": "영찬",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08284490542508903619"
     },
     "user_tz": -540
    },
    "id": "oAcybo6LoyRj",
    "outputId": "13aa85fa-69d3-464f-d8c8-8fc7fd6abfb2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(3)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMpRoz+5f/khjmKtr/GKjBQ",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "심층 CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
