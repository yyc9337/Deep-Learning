{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "('snli_1.0_train.ko.tsv', <http.client.HTTPMessage at 0x15bd3604348>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/multinli.train.ko.tsv\", filename=\"multinli.train.ko.tsv\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/snli_1.0_train.ko.tsv\", filename=\"snli_1.0_train.ko.tsv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "('xnli.dev.ko.tsv', <http.client.HTTPMessage at 0x15bd3476f08>)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검증 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/xnli.dev.ko.tsv\", filename=\"xnli.dev.ko.tsv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "('xnli.test.ko.tsv', <http.client.HTTPMessage at 0x15bd349f688>)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/kakaobrain/KorNLUDatasets/master/KorNLI/xnli.test.ko.tsv\", filename=\"xnli.test.ko.tsv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_snli = pd.read_csv(\"snli_1.0_train.ko.tsv\", sep='\\t', quoting=3)\n",
    "train_xnli = pd.read_csv(\"multinli.train.ko.tsv\", sep='\\t', quoting=3)\n",
    "val_data = pd.read_csv(\"xnli.dev.ko.tsv\", sep='\\t', quoting=3)\n",
    "test_data = pd.read_csv(\"xnli.test.ko.tsv\", sep='\\t', quoting=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                    sentence1                  sentence2     gold_label\n0  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.  한 사람이 경쟁을 위해 말을 훈련시키고 있다.        neutral\n1  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.   한 사람이 식당에서 오믈렛을 주문하고 있다.  contradiction\n2  말을 탄 사람이 고장난 비행기 위로 뛰어오른다.         사람은 야외에서 말을 타고 있다.     entailment\n3          카메라에 웃고 손을 흔드는 아이들          그들은 부모님을 보고 웃고 있다        neutral\n4          카메라에 웃고 손을 흔드는 아이들                    아이들이 있다     entailment",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n      <th>gold_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n      <td>한 사람이 경쟁을 위해 말을 훈련시키고 있다.</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n      <td>한 사람이 식당에서 오믈렛을 주문하고 있다.</td>\n      <td>contradiction</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>말을 탄 사람이 고장난 비행기 위로 뛰어오른다.</td>\n      <td>사람은 야외에서 말을 타고 있다.</td>\n      <td>entailment</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>카메라에 웃고 손을 흔드는 아이들</td>\n      <td>그들은 부모님을 보고 웃고 있다</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>카메라에 웃고 손을 흔드는 아이들</td>\n      <td>아이들이 있다</td>\n      <td>entailment</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_snli.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pre-processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# 결합 후 섞기\n",
    "train_data = train_snli.append(train_xnli)\n",
    "train_data = train_data.sample(frac=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "## 증복데이터 제거\n",
    "def drop_na_and_duplciates(df):\n",
    "  df = df.dropna()\n",
    "  df = df.drop_duplicates()\n",
    "  df = df.reset_index(drop=True)\n",
    "  return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# 결측값 및 중복 샘플 제거\n",
    "train_data = drop_na_and_duplciates(train_data)\n",
    "val_data = drop_na_and_duplciates(val_data)\n",
    "test_data = drop_na_and_duplciates(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        sentence1  \\\n0                             해변의 모래에서 노는 개 두 마리.   \n1              한 무리의 어린 소녀들이 나이든 여자와 이야기를 나누고 있다.   \n2                     내가 널 잃는다고 했잖아 넌 점점 희미해지고 있어   \n3                구명조끼를 입은 일꾼들이 카메라를 향해 미소를 짓고 있다.   \n4       흥분하고 웃는 여자가 한쪽에 붐박스를 들고 있을 때 빨간 난간에 서 있다.   \n...                                           ...   \n941809                             사람이 길을 건너고 있다.   \n941810                          이제 4대 1로 줄 수 있어요.   \n941811              이 사진은 자전거에 색깔이 튀는 흑백으로 편집되었다.   \n941812                             흔적을 남긴 빨간 비행기.   \n941813                      한 남자가 밤에 흙자전거를 뛰어넘는다.   \n\n                                    sentence2     gold_label  \n0                              개들이 사냥을 하고 있다.        neutral  \n1                         한 무리의 소녀들이 잡지를 읽는다.  contradiction  \n2       내가 널 잃는다고 했잖아, 넌 이제 달라졌어. 넌 예전 같지 않아.        neutral  \n3                                       웃는 인간     entailment  \n4                               키가 큰 인간 스타닌드.        neutral  \n...                                       ...            ...  \n941809                          그 사람은 야외에 있다.     entailment  \n941810          지금 이 순간에 2+2 대 1을 허락해 주시겠습니까?     entailment  \n941811                        그 사진은 집의 사진입니다.  contradiction  \n941812               비행기 한 대가 마하 5 속도로 달리고 있다        neutral  \n941813                  한 남자가 자전거를 타고 커브를 돌다.  contradiction  \n\n[941814 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n      <th>gold_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>해변의 모래에서 노는 개 두 마리.</td>\n      <td>개들이 사냥을 하고 있다.</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>한 무리의 어린 소녀들이 나이든 여자와 이야기를 나누고 있다.</td>\n      <td>한 무리의 소녀들이 잡지를 읽는다.</td>\n      <td>contradiction</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>내가 널 잃는다고 했잖아 넌 점점 희미해지고 있어</td>\n      <td>내가 널 잃는다고 했잖아, 넌 이제 달라졌어. 넌 예전 같지 않아.</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>구명조끼를 입은 일꾼들이 카메라를 향해 미소를 짓고 있다.</td>\n      <td>웃는 인간</td>\n      <td>entailment</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>흥분하고 웃는 여자가 한쪽에 붐박스를 들고 있을 때 빨간 난간에 서 있다.</td>\n      <td>키가 큰 인간 스타닌드.</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>941809</th>\n      <td>사람이 길을 건너고 있다.</td>\n      <td>그 사람은 야외에 있다.</td>\n      <td>entailment</td>\n    </tr>\n    <tr>\n      <th>941810</th>\n      <td>이제 4대 1로 줄 수 있어요.</td>\n      <td>지금 이 순간에 2+2 대 1을 허락해 주시겠습니까?</td>\n      <td>entailment</td>\n    </tr>\n    <tr>\n      <th>941811</th>\n      <td>이 사진은 자전거에 색깔이 튀는 흑백으로 편집되었다.</td>\n      <td>그 사진은 집의 사진입니다.</td>\n      <td>contradiction</td>\n    </tr>\n    <tr>\n      <th>941812</th>\n      <td>흔적을 남긴 빨간 비행기.</td>\n      <td>비행기 한 대가 마하 5 속도로 달리고 있다</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>941813</th>\n      <td>한 남자가 밤에 흙자전거를 뛰어넘는다.</td>\n      <td>한 남자가 자전거를 타고 커브를 돌다.</td>\n      <td>contradiction</td>\n    </tr>\n  </tbody>\n</table>\n<p>941814 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              sentence1  \\\n0                              그리고 그가 말했다, \"엄마, 저 왔어요.\"   \n1                              그리고 그가 말했다, \"엄마, 저 왔어요.\"   \n2                              그리고 그가 말했다, \"엄마, 저 왔어요.\"   \n3     내가 무엇을 위해 가고 있는지 또는 어떤 것을 위해 있는지 몰랐기 때문에 워싱턴의 ...   \n4     내가 무엇을 위해 가고 있는지 또는 어떤 것을 위해 있는지 몰랐기 때문에 워싱턴의 ...   \n...                                                 ...   \n2485     피스티는 피즐처럼 중간 영어의 피스틴으로 시작되어 방귀를 뀌기 위해 주먹을 쥐었다.   \n2486     피스티는 피즐처럼 중간 영어의 피스틴으로 시작되어 방귀를 뀌기 위해 주먹을 쥐었다.   \n2487                   진술이 더 나은 반면, 대답은 완성의 정신적 그림을 준다.   \n2488                   진술이 더 나은 반면, 대답은 완성의 정신적 그림을 준다.   \n2489                   진술이 더 나은 반면, 대답은 완성의 정신적 그림을 준다.   \n\n                                              sentence2     gold_label  \n0                     그는 학교 버스가 그를 내려주자마자 엄마에게 전화를 걸었다.        neutral  \n1                                       그는 한마디도 하지 않았다.  contradiction  \n2                                   그는 엄마에게 집에 갔다고 말했다.     entailment  \n3     나는 워싱턴에 가본 적이 없어서 거기 배정을 받았을 때 그 장소를 찾으려다가 길을 ...        neutral  \n4                  워싱턴으로 진군하면서 해야 할 일이 무엇인지 정확히 알고 있었다.  contradiction  \n...                                                 ...            ...  \n2485                            Fiesty는 100년 동안 존재해 왔다.        neutral  \n2486                                Fiesty는 주먹질과는 무관하다.  contradiction  \n2487                                진술은 더 자세한 내용을 알려준다.        neutral  \n2488                                   진술이 더 나은 것은 아니다.  contradiction  \n2489                                          진술이 더 좋다.     entailment  \n\n[2490 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n      <th>gold_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>그리고 그가 말했다, \"엄마, 저 왔어요.\"</td>\n      <td>그는 학교 버스가 그를 내려주자마자 엄마에게 전화를 걸었다.</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>그리고 그가 말했다, \"엄마, 저 왔어요.\"</td>\n      <td>그는 한마디도 하지 않았다.</td>\n      <td>contradiction</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>그리고 그가 말했다, \"엄마, 저 왔어요.\"</td>\n      <td>그는 엄마에게 집에 갔다고 말했다.</td>\n      <td>entailment</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>내가 무엇을 위해 가고 있는지 또는 어떤 것을 위해 있는지 몰랐기 때문에 워싱턴의 ...</td>\n      <td>나는 워싱턴에 가본 적이 없어서 거기 배정을 받았을 때 그 장소를 찾으려다가 길을 ...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>내가 무엇을 위해 가고 있는지 또는 어떤 것을 위해 있는지 몰랐기 때문에 워싱턴의 ...</td>\n      <td>워싱턴으로 진군하면서 해야 할 일이 무엇인지 정확히 알고 있었다.</td>\n      <td>contradiction</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2485</th>\n      <td>피스티는 피즐처럼 중간 영어의 피스틴으로 시작되어 방귀를 뀌기 위해 주먹을 쥐었다.</td>\n      <td>Fiesty는 100년 동안 존재해 왔다.</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>2486</th>\n      <td>피스티는 피즐처럼 중간 영어의 피스틴으로 시작되어 방귀를 뀌기 위해 주먹을 쥐었다.</td>\n      <td>Fiesty는 주먹질과는 무관하다.</td>\n      <td>contradiction</td>\n    </tr>\n    <tr>\n      <th>2487</th>\n      <td>진술이 더 나은 반면, 대답은 완성의 정신적 그림을 준다.</td>\n      <td>진술은 더 자세한 내용을 알려준다.</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>2488</th>\n      <td>진술이 더 나은 반면, 대답은 완성의 정신적 그림을 준다.</td>\n      <td>진술이 더 나은 것은 아니다.</td>\n      <td>contradiction</td>\n    </tr>\n    <tr>\n      <th>2489</th>\n      <td>진술이 더 나은 반면, 대답은 완성의 정신적 그림을 준다.</td>\n      <td>진술이 더 좋다.</td>\n      <td>entailment</td>\n    </tr>\n  </tbody>\n</table>\n<p>2490 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### BERT 토크나이저 적용"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "max_seq_len = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def convert_examples_to_features(sent_list1, sent_list2, max_seq_len, tokenizer):\n",
    "\n",
    "    input_ids, attention_masks, token_type_ids = [], [], []\n",
    "\n",
    "    for sent1, sent2 in tqdm(zip(sent_list1, sent_list2), total=len(sent_list1)):\n",
    "        encoding_result = tokenizer.encode_plus(sent1, sent2, max_length=max_seq_len, pad_to_max_length=True)\n",
    "\n",
    "        input_ids.append(encoding_result['input_ids'])\n",
    "        attention_masks.append(encoding_result['attention_mask'])\n",
    "        token_type_ids.append(encoding_result['token_type_ids'])\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/941814 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\inp032\\.conda\\envs\\yyc\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "  0%|          | 315/941814 [00:00<09:30, 1649.47it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  0%|          | 1936/941814 [00:01<09:54, 1581.64it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  0%|          | 3489/941814 [00:02<10:38, 1468.54it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  0%|          | 3859/941814 [00:02<09:25, 1659.97it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  0%|          | 4027/941814 [00:02<09:22, 1665.83it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  0%|          | 4597/941814 [00:03<14:43, 1060.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|          | 5061/941814 [00:03<17:19, 901.05it/s] Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|          | 5191/941814 [00:03<15:39, 997.08it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|          | 6173/941814 [00:04<10:11, 1530.51it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|          | 6475/941814 [00:04<11:14, 1385.97it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|          | 6616/941814 [00:04<11:30, 1355.18it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|          | 8435/941814 [00:06<12:10, 1278.36it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|          | 8973/941814 [00:06<12:11, 1275.80it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|          | 9765/941814 [00:07<10:33, 1470.91it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|          | 10465/941814 [00:07<09:06, 1703.94it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|          | 10647/941814 [00:07<08:57, 1733.14it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|          | 11131/941814 [00:08<11:06, 1396.84it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|▏         | 11953/941814 [00:08<10:37, 1459.10it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|▏         | 12432/941814 [00:08<10:47, 1434.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|▏         | 12948/941814 [00:09<10:03, 1538.94it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  1%|▏         | 13704/941814 [00:09<11:45, 1316.17it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 14275/941814 [00:10<10:59, 1407.09it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 15736/941814 [00:11<08:09, 1890.74it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 15936/941814 [00:11<08:02, 1917.80it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 17202/941814 [00:11<07:35, 2030.84it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 17406/941814 [00:11<07:45, 1987.08it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 17816/941814 [00:12<07:51, 1958.43it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 18389/941814 [00:12<09:09, 1679.14it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 18707/941814 [00:12<12:29, 1231.09it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 18839/941814 [00:12<13:39, 1126.68it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 19537/941814 [00:13<11:20, 1355.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 20457/941814 [00:14<10:26, 1469.94it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 20790/941814 [00:14<10:26, 1470.01it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 20939/941814 [00:14<10:45, 1427.23it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 21222/941814 [00:14<11:17, 1358.95it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 22075/941814 [00:15<11:24, 1343.17it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 22485/941814 [00:15<09:02, 1693.98it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 23054/941814 [00:15<09:29, 1614.66it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  2%|▏         | 23220/941814 [00:15<09:24, 1627.02it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 24644/941814 [00:16<09:17, 1644.10it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 24810/941814 [00:16<09:20, 1634.78it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 25644/941814 [00:17<12:45, 1196.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 25768/941814 [00:17<12:42, 1201.65it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 26103/941814 [00:17<10:43, 1423.53it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 26531/941814 [00:18<11:36, 1314.09it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 26803/941814 [00:18<11:42, 1302.94it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 27182/941814 [00:18<12:41, 1200.99it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 27871/941814 [00:19<12:02, 1265.27it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 28038/941814 [00:19<11:03, 1376.19it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 28191/941814 [00:19<10:45, 1416.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 28514/941814 [00:19<10:03, 1513.15it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 28667/941814 [00:19<10:10, 1496.12it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 28818/941814 [00:20<10:28, 1453.16it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 28981/941814 [00:20<10:10, 1495.81it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 29725/941814 [00:20<08:44, 1738.63it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 29900/941814 [00:20<09:19, 1631.28it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 30241/941814 [00:20<09:29, 1600.82it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 30612/941814 [00:21<08:49, 1720.65it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 31273/941814 [00:21<09:40, 1568.31it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 31625/941814 [00:21<09:12, 1646.19it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 32185/941814 [00:22<08:32, 1775.32it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 32525/941814 [00:22<09:47, 1546.43it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  3%|▎         | 32692/941814 [00:22<09:35, 1580.21it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▎         | 33315/941814 [00:22<07:57, 1903.07it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▎         | 33508/941814 [00:22<08:24, 1800.40it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▎         | 34065/941814 [00:23<09:02, 1673.76it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▎         | 34556/941814 [00:23<09:40, 1563.21it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▎         | 34867/941814 [00:23<10:18, 1465.94it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▎         | 35305/941814 [00:24<15:14, 991.51it/s] Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 35753/941814 [00:24<11:30, 1312.02it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 36097/941814 [00:24<10:04, 1497.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 36655/941814 [00:25<12:19, 1223.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 36931/941814 [00:25<11:35, 1300.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 38039/941814 [00:26<10:10, 1480.99it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 38640/941814 [00:26<08:12, 1834.65it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 38852/941814 [00:26<07:51, 1913.85it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 39271/941814 [00:26<08:43, 1724.78it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 39451/941814 [00:26<09:06, 1651.59it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 40788/941814 [00:27<08:38, 1737.00it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 40989/941814 [00:27<08:16, 1813.71it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 41359/941814 [00:28<08:12, 1827.91it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 41560/941814 [00:28<07:59, 1875.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 42147/941814 [00:28<07:47, 1925.62it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  4%|▍         | 42353/941814 [00:28<07:38, 1959.78it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  5%|▍         | 43381/941814 [00:29<07:29, 1996.89it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  5%|▍         | 44578/941814 [00:29<07:44, 1930.80it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  5%|▍         | 44988/941814 [00:29<07:42, 1940.44it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  5%|▍         | 45184/941814 [00:30<08:40, 1722.03it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  5%|▍         | 46229/941814 [00:30<09:33, 1562.34it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  5%|▌         | 47682/941814 [00:31<08:45, 1700.90it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  5%|▌         | 47857/941814 [00:31<08:42, 1710.04it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  5%|▌         | 48506/941814 [00:32<10:01, 1484.41it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  5%|▌         | 48833/941814 [00:32<09:34, 1555.35it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  5%|▌         | 50563/941814 [00:33<09:30, 1562.88it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  5%|▌         | 51291/941814 [00:33<08:22, 1773.05it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  5%|▌         | 51477/941814 [00:33<08:16, 1793.60it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▌         | 51862/941814 [00:34<07:59, 1857.17it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▌         | 52049/941814 [00:34<08:03, 1839.32it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▌         | 52602/941814 [00:34<08:20, 1775.17it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▌         | 53006/941814 [00:34<07:47, 1900.15it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▌         | 53220/941814 [00:34<07:31, 1969.22it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▌         | 54486/941814 [00:35<07:19, 2016.73it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▌         | 54693/941814 [00:35<07:16, 2032.21it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▌         | 55308/941814 [00:35<07:18, 2023.84it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▌         | 56169/941814 [00:36<07:53, 1870.63it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▌         | 56695/941814 [00:36<10:50, 1360.49it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▌         | 57537/941814 [00:37<08:43, 1689.75it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▌         | 57710/941814 [00:37<09:13, 1597.08it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▌         | 58033/941814 [00:37<09:37, 1529.91it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▌         | 58702/941814 [00:38<08:55, 1649.85it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▋         | 59821/941814 [00:38<07:58, 1843.35it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▋         | 60012/941814 [00:38<07:54, 1857.61it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▋         | 60219/941814 [00:38<07:39, 1920.43it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▋         | 60621/941814 [00:39<07:28, 1964.67it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  6%|▋         | 61051/941814 [00:39<07:09, 2052.62it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  7%|▋         | 62578/941814 [00:40<08:03, 1819.18it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  7%|▋         | 63906/941814 [00:40<08:30, 1721.09it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  7%|▋         | 64508/941814 [00:41<07:40, 1906.76it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  7%|▋         | 66579/941814 [00:42<07:18, 1995.95it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  7%|▋         | 66780/941814 [00:42<07:18, 1994.45it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  7%|▋         | 68426/941814 [00:43<09:10, 1586.83it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  7%|▋         | 68993/941814 [00:43<08:13, 1768.91it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  7%|▋         | 69199/941814 [00:43<07:52, 1848.69it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  7%|▋         | 69413/941814 [00:43<07:32, 1928.57it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  7%|▋         | 70001/941814 [00:44<07:34, 1919.26it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  7%|▋         | 70205/941814 [00:44<07:27, 1949.38it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "  7%|▋         | 70536/941814 [00:44<09:09, 1584.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8360/1110682919.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mX_train\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconvert_examples_to_features\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'sentence1'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'sentence2'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_seq_len\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmax_seq_len\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8360/3005930689.py\u001B[0m in \u001B[0;36mconvert_examples_to_features\u001B[1;34m(sent_list1, sent_list2, max_seq_len, tokenizer)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0msent1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msent2\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msent_list1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msent_list2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msent_list1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m         \u001B[0mencoding_result\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencode_plus\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msent1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msent2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmax_seq_len\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpad_to_max_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[0minput_ids\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoding_result\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'input_ids'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\yyc\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001B[0m in \u001B[0;36mencode_plus\u001B[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m   2552\u001B[0m             \u001B[0mreturn_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreturn_length\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2553\u001B[0m             \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2554\u001B[1;33m             \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2555\u001B[0m         )\n\u001B[0;32m   2556\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\yyc\\lib\\site-packages\\transformers\\tokenization_utils.py\u001B[0m in \u001B[0;36m_encode_plus\u001B[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    645\u001B[0m             )\n\u001B[0;32m    646\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 647\u001B[1;33m         \u001B[0mfirst_ids\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_input_ids\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    648\u001B[0m         \u001B[0msecond_ids\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_input_ids\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext_pair\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mtext_pair\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    649\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\yyc\\lib\\site-packages\\transformers\\tokenization_utils.py\u001B[0m in \u001B[0;36mget_input_ids\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m    614\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mget_input_ids\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    615\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 616\u001B[1;33m                 \u001B[0mtokens\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    617\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_tokens_to_ids\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokens\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    618\u001B[0m             \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\yyc\\lib\\site-packages\\transformers\\tokenization_utils.py\u001B[0m in \u001B[0;36mtokenize\u001B[1;34m(self, text, **kwargs)\u001B[0m\n\u001B[0;32m    545\u001B[0m                 \u001B[0mtokenized_text\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    546\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 547\u001B[1;33m                 \u001B[0mtokenized_text\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    548\u001B[0m         \u001B[1;31m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    549\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mtokenized_text\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\yyc\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py\u001B[0m in \u001B[0;36m_tokenize\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m    228\u001B[0m                     \u001B[0msplit_tokens\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    229\u001B[0m                 \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 230\u001B[1;33m                     \u001B[0msplit_tokens\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwordpiece_tokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    231\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    232\u001B[0m             \u001B[0msplit_tokens\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwordpiece_tokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\yyc\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py\u001B[0m in \u001B[0;36mtokenize\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m    530\u001B[0m             \u001B[0mis_bad\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    531\u001B[0m             \u001B[0mstart\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 532\u001B[1;33m             \u001B[0msub_tokens\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    533\u001B[0m             \u001B[1;32mwhile\u001B[0m \u001B[0mstart\u001B[0m \u001B[1;33m<\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mchars\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    534\u001B[0m                 \u001B[0mend\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mchars\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "X_train = convert_examples_to_features(train_data['sentence1'], train_data['sentence2'], max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 최대 길이: 128\n",
    "input_id = X_train[0][0]\n",
    "attention_mask = X_train[1][0]\n",
    "token_type_id = X_train[2][0]\n",
    "\n",
    "print('단어에 대한 정수 인코딩 :',input_id)\n",
    "print('어텐션 마스크 :',attention_mask)\n",
    "print('세그먼트 인코딩 :',token_type_id)\n",
    "print('각 인코딩의 길이 :', len(input_id))\n",
    "print('정수 인코딩 복원 :',tokenizer.decode(input_id))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_val = convert_examples_to_features(val_data['sentence1'], val_data['sentence2'], max_seq_len=max_seq_len, tokenizer=tokenizer)\n",
    "X_test = convert_examples_to_features(test_data['sentence1'], test_data['sentence2'], max_seq_len=max_seq_len, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TPU 작동을 위한 코드\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  model = TFBertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=3, from_pt=True)\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "  model.compile(optimizer=optimizer, loss=model.compute_loss, metrics = ['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "yyc",
   "language": "python",
   "display_name": "yyc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}