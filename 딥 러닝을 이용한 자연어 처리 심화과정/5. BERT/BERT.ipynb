{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## BERT(Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "\n",
    "트랜스포머(transformer)의 등장 이후, 다양한 자연어 처리 태스크에서 사용되었던 RNN 계열의 신경망인 LSTM, GRU는 트랜스포머로 대체되어가는 추세입니다. 이에 따라 다양한 트랜스포머 계열의 BERT, GPT, T5 등 다양한 사전 훈련된 언어 모델들이 계속해서 등장하고 있습니다. 여기서는 가장 유명한 사전 훈련된 언어 모델 중 하나인 BERT에 대해서 소개합니다. 이번 챕터에서 소개하는 BERT의 기본 개념을 이해하고, ALBERT, RoBERTa, ELECTRA와 같은 BERT의 파생 모델을 공부하는 방식으로 트랜스포머 계열 언어 모델에 대한 공부를 이어나가시기 바랍니다.\n",
    "\n",
    "\n",
    "BERT(Bidirectional Encoder Representations from Transformers)는 2018년에 구글이 공개한 사전 훈련된 모델입니다. BERT라는 이름은 세서미 스트리트라는 미국 인형극의 케릭터 이름이기도 한데, 앞서 소개한 임베딩 방법론인 ELMo와 마찬가지로 세서미 스트리트의 케릭터 이름을 따온 것이기도 합니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "BERT가 높은 성능을 얻을 수 있었던 것은, 레이블이 없는 방대한 데이터로 사전 훈련된 모델을 가지고, 레이블이 있는 다른 작업(Task)에서 추가 훈련과 함께 하이퍼파라미터를 재조정하여 이 모델을 사용하면 성능이 높게 나오는 기존의 사례들을 참고하였기 때문입니다. 다른 작업에 대해서 파라미터 재조정을 위한 추가 훈련 과정을 파인 튜닝(Fine-tuning)이라고 합니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}