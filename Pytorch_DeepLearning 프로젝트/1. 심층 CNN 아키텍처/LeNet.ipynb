{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.quantized' has no attribute 'Dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/innerwave/ailab/YYC/Pytorch_DeepLearning 프로젝트/1. 심층 CNN 아키텍처/LeNet.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Binnerwave@office.innerwave.co.kr:50023/home/innerwave/ailab/YYC/Pytorch_DeepLearning%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3/1.%20%EC%8B%AC%EC%B8%B5%20CNN%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/LeNet.ipynb#ch0000004vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Binnerwave@office.innerwave.co.kr:50023/home/innerwave/ailab/YYC/Pytorch_DeepLearning%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3/1.%20%EC%8B%AC%EC%B8%B5%20CNN%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/LeNet.ipynb#ch0000004vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Binnerwave@office.innerwave.co.kr:50023/home/innerwave/ailab/YYC/Pytorch_DeepLearning%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3/1.%20%EC%8B%AC%EC%B8%B5%20CNN%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/LeNet.ipynb#ch0000004vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Binnerwave@office.innerwave.co.kr:50023/home/innerwave/ailab/YYC/Pytorch_DeepLearning%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3/1.%20%EC%8B%AC%EC%B8%B5%20CNN%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/LeNet.ipynb#ch0000004vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Binnerwave@office.innerwave.co.kr:50023/home/innerwave/ailab/YYC/Pytorch_DeepLearning%20%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3/1.%20%EC%8B%AC%EC%B8%B5%20CNN%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/LeNet.ipynb#ch0000004vscode-remote?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/__init__.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m io\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m feature_extraction\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m optical_flow\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m quantization\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m segmentation\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m video\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/quantization/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmobilenet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresnet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgooglenet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/quantization/mobilenet.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmobilenetv2\u001b[39;00m \u001b[39mimport\u001b[39;00m QuantizableMobileNetV2, mobilenet_v2, __all__ \u001b[39mas\u001b[39;00m mv2_all\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmobilenetv3\u001b[39;00m \u001b[39mimport\u001b[39;00m QuantizableMobileNetV3, mobilenet_v3_large, __all__ \u001b[39mas\u001b[39;00m mv3_all\n\u001b[1;32m      4\u001b[0m __all__ \u001b[39m=\u001b[39m mv2_all \u001b[39m+\u001b[39m mv3_all\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/quantization/mobilenetv2.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mao\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantization\u001b[39;00m \u001b[39mimport\u001b[39;00m QuantStub, DeQuantStub\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmobilenetv2\u001b[39;00m \u001b[39mimport\u001b[39;00m InvertedResidual, MobileNetV2, model_urls\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_internally_replaced_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_state_dict_from_url\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/ao/quantization/__init__.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mqconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mquant_type\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mquantization_mappings\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mquantize\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mquantize_jit\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/ao/quantization/quantization_mappings.py:41\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m DEFAULT_REFERENCE_STATIC_QUANT_MODULE_MAPPINGS : Dict[Callable, Any] \u001b[39m=\u001b[39m {\n\u001b[1;32m     29\u001b[0m     nn\u001b[39m.\u001b[39mLinear: nnqr\u001b[39m.\u001b[39mLinear,\n\u001b[1;32m     30\u001b[0m     nn\u001b[39m.\u001b[39mConv1d: nnqr\u001b[39m.\u001b[39mConv1d,\n\u001b[1;32m     31\u001b[0m     nn\u001b[39m.\u001b[39mConv2d: nnqr\u001b[39m.\u001b[39mConv2d,\n\u001b[1;32m     32\u001b[0m     nn\u001b[39m.\u001b[39mConv3d: nnqr\u001b[39m.\u001b[39mConv3d,\n\u001b[1;32m     33\u001b[0m }\n\u001b[1;32m     35\u001b[0m \u001b[39m# Default map for swapping float module to quantized ones\u001b[39;00m\n\u001b[1;32m     36\u001b[0m DEFAULT_STATIC_QUANT_MODULE_MAPPINGS : Dict[Callable, Any] \u001b[39m=\u001b[39m {\n\u001b[1;32m     37\u001b[0m     QuantStub: nnq\u001b[39m.\u001b[39mQuantize,\n\u001b[1;32m     38\u001b[0m     DeQuantStub: nnq\u001b[39m.\u001b[39mDeQuantize,\n\u001b[1;32m     39\u001b[0m     nn\u001b[39m.\u001b[39mBatchNorm2d: nnq\u001b[39m.\u001b[39mBatchNorm2d,\n\u001b[1;32m     40\u001b[0m     nn\u001b[39m.\u001b[39mBatchNorm3d: nnq\u001b[39m.\u001b[39mBatchNorm3d,\n\u001b[0;32m---> 41\u001b[0m     nn\u001b[39m.\u001b[39mDropout: nnq\u001b[39m.\u001b[39;49mDropout,\n\u001b[1;32m     42\u001b[0m     nn\u001b[39m.\u001b[39mConv1d: nnq\u001b[39m.\u001b[39mConv1d,\n\u001b[1;32m     43\u001b[0m     nn\u001b[39m.\u001b[39mConv2d: nnq\u001b[39m.\u001b[39mConv2d,\n\u001b[1;32m     44\u001b[0m     nn\u001b[39m.\u001b[39mConv3d: nnq\u001b[39m.\u001b[39mConv3d,\n\u001b[1;32m     45\u001b[0m     nn\u001b[39m.\u001b[39mConvTranspose1d: nnq\u001b[39m.\u001b[39mConvTranspose1d,\n\u001b[1;32m     46\u001b[0m     nn\u001b[39m.\u001b[39mConvTranspose2d: nnq\u001b[39m.\u001b[39mConvTranspose2d,\n\u001b[1;32m     47\u001b[0m     nn\u001b[39m.\u001b[39mConvTranspose3d: nnq\u001b[39m.\u001b[39mConvTranspose3d,\n\u001b[1;32m     48\u001b[0m     nn\u001b[39m.\u001b[39mELU: nnq\u001b[39m.\u001b[39mELU,\n\u001b[1;32m     49\u001b[0m     nn\u001b[39m.\u001b[39mEmbedding: nnq\u001b[39m.\u001b[39mEmbedding,\n\u001b[1;32m     50\u001b[0m     nn\u001b[39m.\u001b[39mEmbeddingBag: nnq\u001b[39m.\u001b[39mEmbeddingBag,\n\u001b[1;32m     51\u001b[0m     nn\u001b[39m.\u001b[39mGroupNorm: nnq\u001b[39m.\u001b[39mGroupNorm,\n\u001b[1;32m     52\u001b[0m     nn\u001b[39m.\u001b[39mHardswish: nnq\u001b[39m.\u001b[39mHardswish,\n\u001b[1;32m     53\u001b[0m     nn\u001b[39m.\u001b[39mInstanceNorm1d: nnq\u001b[39m.\u001b[39mInstanceNorm1d,\n\u001b[1;32m     54\u001b[0m     nn\u001b[39m.\u001b[39mInstanceNorm2d: nnq\u001b[39m.\u001b[39mInstanceNorm2d,\n\u001b[1;32m     55\u001b[0m     nn\u001b[39m.\u001b[39mInstanceNorm3d: nnq\u001b[39m.\u001b[39mInstanceNorm3d,\n\u001b[1;32m     56\u001b[0m     nn\u001b[39m.\u001b[39mLayerNorm: nnq\u001b[39m.\u001b[39mLayerNorm,\n\u001b[1;32m     57\u001b[0m     nn\u001b[39m.\u001b[39mLeakyReLU: nnq\u001b[39m.\u001b[39mLeakyReLU,\n\u001b[1;32m     58\u001b[0m     nn\u001b[39m.\u001b[39mmodules\u001b[39m.\u001b[39mlinear\u001b[39m.\u001b[39mNonDynamicallyQuantizableLinear: nnq\u001b[39m.\u001b[39mLinear,\n\u001b[1;32m     59\u001b[0m     nn\u001b[39m.\u001b[39mLinear: nnq\u001b[39m.\u001b[39mLinear,\n\u001b[1;32m     60\u001b[0m     nn\u001b[39m.\u001b[39mReLU6: nnq\u001b[39m.\u001b[39mReLU6,\n\u001b[1;32m     61\u001b[0m     nn\u001b[39m.\u001b[39mDropout: nnq\u001b[39m.\u001b[39mDropout,\n\u001b[1;32m     62\u001b[0m     \u001b[39m# Wrapper Modules:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     nnq\u001b[39m.\u001b[39mFloatFunctional: nnq\u001b[39m.\u001b[39mQFunctional,\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Intrinsic modules:\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     nni\u001b[39m.\u001b[39mBNReLU2d: nniq\u001b[39m.\u001b[39mBNReLU2d,\n\u001b[1;32m     66\u001b[0m     nni\u001b[39m.\u001b[39mBNReLU3d: nniq\u001b[39m.\u001b[39mBNReLU3d,\n\u001b[1;32m     67\u001b[0m     nni\u001b[39m.\u001b[39mConvReLU1d: nniq\u001b[39m.\u001b[39mConvReLU1d,\n\u001b[1;32m     68\u001b[0m     nni\u001b[39m.\u001b[39mConvReLU2d: nniq\u001b[39m.\u001b[39mConvReLU2d,\n\u001b[1;32m     69\u001b[0m     nni\u001b[39m.\u001b[39mConvReLU3d: nniq\u001b[39m.\u001b[39mConvReLU3d,\n\u001b[1;32m     70\u001b[0m     nni\u001b[39m.\u001b[39mLinearReLU: nniq\u001b[39m.\u001b[39mLinearReLU,\n\u001b[1;32m     71\u001b[0m     nniqat\u001b[39m.\u001b[39mConvBn1d: nnq\u001b[39m.\u001b[39mConv1d,\n\u001b[1;32m     72\u001b[0m     nniqat\u001b[39m.\u001b[39mConvBn2d: nnq\u001b[39m.\u001b[39mConv2d,\n\u001b[1;32m     73\u001b[0m     nniqat\u001b[39m.\u001b[39mConvBn3d: nnq\u001b[39m.\u001b[39mConv3d,\n\u001b[1;32m     74\u001b[0m     nniqat\u001b[39m.\u001b[39mConvBnReLU1d: nniq\u001b[39m.\u001b[39mConvReLU1d,\n\u001b[1;32m     75\u001b[0m     nniqat\u001b[39m.\u001b[39mConvBnReLU2d: nniq\u001b[39m.\u001b[39mConvReLU2d,\n\u001b[1;32m     76\u001b[0m     nniqat\u001b[39m.\u001b[39mConvBnReLU3d: nniq\u001b[39m.\u001b[39mConvReLU3d,\n\u001b[1;32m     77\u001b[0m     nniqat\u001b[39m.\u001b[39mConvReLU2d: nniq\u001b[39m.\u001b[39mConvReLU2d,\n\u001b[1;32m     78\u001b[0m     nniqat\u001b[39m.\u001b[39mConvReLU3d: nniq\u001b[39m.\u001b[39mConvReLU3d,\n\u001b[1;32m     79\u001b[0m     nniqat\u001b[39m.\u001b[39mLinearReLU: nniq\u001b[39m.\u001b[39mLinearReLU,\n\u001b[1;32m     80\u001b[0m     \u001b[39m# QAT modules:\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     nnqat\u001b[39m.\u001b[39mLinear: nnq\u001b[39m.\u001b[39mLinear,\n\u001b[1;32m     82\u001b[0m     nnqat\u001b[39m.\u001b[39mConv2d: nnq\u001b[39m.\u001b[39mConv2d,\n\u001b[1;32m     83\u001b[0m     nnqat\u001b[39m.\u001b[39mConv3d: nnq\u001b[39m.\u001b[39mConv3d,\n\u001b[1;32m     84\u001b[0m }\n\u001b[1;32m     86\u001b[0m \u001b[39m# Default map for swapping float module to qat modules\u001b[39;00m\n\u001b[1;32m     87\u001b[0m DEFAULT_QAT_MODULE_MAPPINGS : Dict[Callable, Any] \u001b[39m=\u001b[39m {\n\u001b[1;32m     88\u001b[0m     nn\u001b[39m.\u001b[39mConv2d: nnqat\u001b[39m.\u001b[39mConv2d,\n\u001b[1;32m     89\u001b[0m     nn\u001b[39m.\u001b[39mConv3d: nnqat\u001b[39m.\u001b[39mConv3d,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     nni\u001b[39m.\u001b[39mLinearReLU: nniqat\u001b[39m.\u001b[39mLinearReLU,\n\u001b[1;32m    102\u001b[0m }\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn.quantized' has no attribute 'Dropout'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.manual_seed(55)\n",
    "\n",
    "# The mean and std are kept as 0.5 for normalizing pixel values as the pixel values are originally in the range 0 to 1\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomCrop(32, 4),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10000, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "# ordering is important\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (cn1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (cn2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 3 input image channel, 6 output feature maps and 5x5 conv kernel\n",
    "        self.cn1 = nn.Conv2d(3, 6, 5)\n",
    "        # 6 input image channel, 16 output feature maps and 5x5 conv kernel\n",
    "        self.cn2 = nn.Conv2d(6, 16, 5)\n",
    "        # fully connected layers of size 120, 84 and 10\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 is the spatial dimension at this layer\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution with 5x5 kernel\n",
    "        x = F.relu(self.cn1(x))\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        # Convolution with 5x5 kernel\n",
    "        x = F.relu(self.cn2(x))\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        # Flatten spatial and depth dimensions into a single vector\n",
    "        x = x.view(-1, self.flattened_features(x))\n",
    "        # Fully connected operations\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    ## 전체 개수를 계산하는데에 쓰임 이 메서드는 특징의 공간적 표현을 단일 숫자 벡터로 평면화하여 완전 연결 계층의 입력으로 사용될 수 있음.\n",
    "    def flattened_features(self, x):\n",
    "        # all except the first (batch) dimension\n",
    "        size = x.size()[1:]\n",
    "        num_feats = 1\n",
    "        for s in size:\n",
    "            num_feats *= s\n",
    "        return num_feats\n",
    "\n",
    "\n",
    "lenet = LeNet()\n",
    "print(lenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, trainloader, optim, epoch):\n",
    "    # initialize loss\n",
    "    loss_total = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # ip refers to the input images, and ground_truth refers to the output classes the images belong to\n",
    "        ip, ground_truth = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # forward pass + backward pass + optimization step\n",
    "        op = net(ip)\n",
    "        loss = nn.CrossEntropyLoss()(op, ground_truth)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # update loss\n",
    "        loss_total += loss.item()\n",
    "\n",
    "        # print loss statistics\n",
    "        if (i+1) % 1000 == 0:    # print at the interval of 1000 mini-batches\n",
    "            print('[Epoch number : %d, Mini-batches: %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss_total / 200))\n",
    "            loss_total = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(net, testloader):\n",
    "    success = 0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            im, ground_truth = data\n",
    "            op = net(im)\n",
    "            _, pred = torch.max(op.data, 1)\n",
    "            counter += ground_truth.size(0)\n",
    "            success += (pred == ground_truth).sum().item()\n",
    "\n",
    "    print('LeNet accuracy on 10000 images from test dataset: %d %%' % (\n",
    "        100 * success / counter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
