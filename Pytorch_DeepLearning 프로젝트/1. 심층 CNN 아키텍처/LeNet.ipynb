{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f86a946dc10>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.manual_seed(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (cn1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (cn2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 3 input image channel, 6 output feature maps and 5x5 conv kernel\n",
    "        self.cn1 = nn.Conv2d(3, 6, 5)\n",
    "        # 6 input image channel, 16 output feature maps and 5x5 conv kernel\n",
    "        self.cn2 = nn.Conv2d(6, 16, 5)\n",
    "        # fully connected layers of size 120, 84 and 10\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 is the spatial dimension at this layer\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution with 5x5 kernel\n",
    "        x = F.relu(self.cn1(x))\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        # Convolution with 5x5 kernel\n",
    "        x = F.relu(self.cn2(x))\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        # Flatten spatial and depth dimensions into a single vector\n",
    "        x = x.view(-1, self.flattened_features(x))\n",
    "        # Fully connected operations\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    ## 전체 개수를 계산하는데에 쓰임 이 메서드는 특징의 공간적 표현을 단일 숫자 벡터로 평면화하여 완전 연결 계층의 입력으로 사용될 수 있음.\n",
    "    def flattened_features(self, x):\n",
    "        # all except the first (batch) dimension\n",
    "        size = x.size()[1:]\n",
    "        num_feats = 1\n",
    "        for s in size:\n",
    "            num_feats *= s\n",
    "        return num_feats\n",
    "\n",
    "\n",
    "lenet = LeNet()\n",
    "print(lenet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def train(net, trainloader, optim, epoch):\n",
    "    # initialize loss\n",
    "    loss_total = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # ip refers to the input images, and ground_truth refers to the output classes the images belong to\n",
    "        ip, ground_truth = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # forward pass + backward pass + optimization step\n",
    "        op = net(ip)\n",
    "        loss = nn.CrossEntropyLoss()(op, ground_truth)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # update loss\n",
    "        loss_total += loss.item()\n",
    "\n",
    "        # print loss statistics\n",
    "        if (i+1) % 1000 == 0:    # print at the interval of 1000 mini-batches\n",
    "            print('[Epoch number : %d, Mini-batches: %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss_total / 200))\n",
    "            loss_total = 0.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test(net, testloader):\n",
    "    success = 0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            im, ground_truth = data\n",
    "            op = net(im)\n",
    "            _, pred = torch.max(op.data, 1)\n",
    "            counter += ground_truth.size(0)\n",
    "            success += (pred == ground_truth).sum().item()\n",
    "\n",
    "    print('LeNet accuracy on 10000 images from test dataset: %d %%' % (\n",
    "        100 * success / counter))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}