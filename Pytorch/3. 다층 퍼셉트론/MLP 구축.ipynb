{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install http://download.pytorch.org/whl/cu90/torch-1.0.0-cp36-cp36m-linux_x86_64.whl\n",
    "# !pip install torchvision\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 손글씨 문자를 판별하는 MLP 작성"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 10)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# optimizer 정의\n",
    "# Loss 함수 정의\n",
    "\n",
    "from torch import optim\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data\n",
    "Y = digits.target\n",
    "\n",
    "# NumPy의 ndarray를 PyTorch의 Tensor로 변환\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.int64)\n",
    "\n",
    "# 소프트맥스 크로스 엔트로피\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam\n",
    "optimizer = optim.Adam(net.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 손실 함수의 로그\n",
    "losses = []\n",
    "\n",
    "# 100회 반복\n",
    "for epoc in range(100):\n",
    "     # backward 메서드로 계산된\n",
    "    #이전 값을 삭제\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 선형 모델로 y의 예측 값 계산\n",
    "    y_pred = net(X)\n",
    "\n",
    "    # MSE loss와 w를 사용한 미분 계산\n",
    "    loss = loss_fn(y_pred, Y)\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사를 갱신\n",
    "    optimizer.step()\n",
    "\n",
    "    # 수렴 확인을 위해 loss를 기록해둔다\n",
    "    losses.append(loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (0): Linear(in_features=64, out_features=32, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=32, out_features=16, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=16, out_features=10, bias=True)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.to('cpu')\n",
    "Y = Y.to(\"cpu\")\n",
    "net.to(\"cpu\")\n",
    "\n",
    "# 이후 처리는 동일하게 optimizer를 설정해서 학습 루프를 돌린다"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61.73985683917999, 42.629998207092285, 24.565683364868164, 14.431542694568634, 9.543607011437416, 7.341830536723137, 5.806207753717899, 5.184779979288578, 4.404019859619439, 3.857878103852272]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "# Dataset 작성\n",
    "ds = TensorDataset(X, Y)\n",
    "\n",
    "# 순서로 섞어서 64개씩 데이터를 반환하는 DataLoader 작성\n",
    "loader = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 10)\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.6329306326806545, 3.1905866088345647, 3.0085658002644777, 2.7208444345742464, 2.6759759299457073, 2.7945490647107363, 2.499379019252956, 2.005095398053527, 1.8312876373529434, 1.6318859700113535]\n"
     ]
    }
   ],
   "source": [
    "# 최적화 실행\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for xx, yy in loader:\n",
    "        # xx, yy는 64개만는다\n",
    "        y_pred = net(xx)\n",
    "        loss = loss_fn(y_pred, yy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    losses.append(running_loss)\n",
    "print(losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 학습 효율화"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#데이터를 훈령용과 검증용으로 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 전체의 30%는 검증용\n",
    "X = digits.data\n",
    "Y = digits.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.int64)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.int64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# 여러 층을 쌓아서 깊은 신경망을 구축한다\n",
    "k = 100\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(64, k),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(k, k),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(k, k),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(k, k),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(k, 10)\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "\n",
    "# 훈련용 데이터로 DataLoader를 작성\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_losses   : [1.4032084910316673e-06, 1.3303913569200467e-06, 1.3186746238092476e-06, 1.2789391291425418e-06, 1.240243873208595e-06, 1.2297151335051474e-06, 1.1757793491649584e-06, 1.1883458759015389e-06, 1.1262160655452392e-06, 1.094174896784294e-06, 1.0686818052490912e-06, 1.0483896980636368e-06, 1.0201372522790673e-06, 1.0116467195349544e-06, 9.790427941862408e-07, 9.58432496580905e-07, 9.290338042342811e-07, 9.148544028775153e-07, 8.863686762794761e-07, 8.935434010481913e-07, 8.694195448136281e-07, 8.457203844814424e-07, 8.257249835453014e-07, 7.95784909330567e-07, 7.931530866391639e-07, 7.350983818008526e-07, 7.197408145943066e-07, 7.089579729433983e-07, 6.887927613459984e-07, 6.734247472596239e-07, 6.605403771759601e-07, 6.418608727369992e-07, 6.292418675002409e-07, 6.104245348964445e-07, 5.97550590023333e-07, 5.801449138067628e-07, 5.6979691419857e-07, 5.593747119748152e-07, 5.45556315974712e-07, 5.460869819929625e-07, 5.176327276556182e-07, 5.065206263766005e-07, 4.950476533776118e-07, 4.915135071369966e-07, 4.7309944124097643e-07, 4.5988591706301844e-07, 4.6296380527512434e-07, 4.4709687913178317e-07, 4.449955255107596e-07, 4.194280653389517e-07, 4.1159538364482637e-07, 4.280992525488104e-07, 3.98328779829317e-07, 3.847756331673771e-07, 3.730161098976973e-07, 3.7127554464451606e-07, 3.595266224902634e-07, 3.5462329915885556e-07, 3.3849102937102185e-07, 3.325475812442045e-07, 3.238128349618108e-07, 3.1996020574900706e-07, 3.090603469044431e-07, 3.006970652873447e-07, 2.934375498598048e-07, 3.2910875237581797e-07, 2.8323816131239124e-07, 2.8124288104090186e-07, 2.70884274100033e-07, 2.663099488168031e-07, 2.571400220501684e-07, 2.5060220863701795e-07, 2.5111168142006755e-07, 2.427165390610042e-07, 2.3326003965980118e-07, 2.3255957742457795e-07, 2.359134076466664e-07, 2.1826342032883215e-07, 2.1525985422075294e-07, 2.0766070208129605e-07, 2.021205552328918e-07, 1.9868182178675087e-07, 1.9936109217028184e-07, 1.9540232675224044e-07, 1.8637037865682933e-07, 1.8140333216192354e-07, 1.7785846714501437e-07, 1.7356007876474112e-07, 1.7496104236627912e-07, 1.6710717413814518e-07, 1.6782888610468768e-07, 1.628300009242477e-07, 1.5340534974511e-07, 1.5249261126771859e-07, 1.5098551429628168e-07, 1.4476609943282224e-07, 1.4149718888316735e-07, 1.3659382882899797e-07, 1.352565402812936e-07, 1.3344166988487743e-07]\n",
      "--------------------------------------------\n",
      "test_losses   : [0.09172806143760681, 0.09100677073001862, 0.09163562953472137, 0.09241397678852081, 0.09219098836183548, 0.0920652225613594, 0.09277217090129852, 0.0927584320306778, 0.093785859644413, 0.0937335193157196, 0.09327389299869537, 0.09342943131923676, 0.09346838295459747, 0.09444010257720947, 0.09369242191314697, 0.09342937916517258, 0.09338574856519699, 0.09354905784130096, 0.09390423446893692, 0.09416303783655167, 0.09459474682807922, 0.0942511186003685, 0.09588806331157684, 0.09496425837278366, 0.09477978199720383, 0.09492506831884384, 0.09521666169166565, 0.09577232599258423, 0.09596012532711029, 0.0962248295545578, 0.09494107961654663, 0.09569276869297028, 0.09629079699516296, 0.09534699469804764, 0.09614826738834381, 0.09625367075204849, 0.09689638763666153, 0.09624484181404114, 0.0964997187256813, 0.09695091843605042, 0.09646579623222351, 0.09710683673620224, 0.09682491421699524, 0.09683243930339813, 0.09858015179634094, 0.09672144800424576, 0.09881400316953659, 0.09872281551361084, 0.09733536094427109, 0.09870139509439468, 0.09821245819330215, 0.0988098680973053, 0.09787117689847946, 0.09901599586009979, 0.0981956198811531, 0.09903780370950699, 0.09971825033426285, 0.09963878244161606, 0.09866316616535187, 0.09917501360177994, 0.09937550872564316, 0.0990033969283104, 0.0999407023191452, 0.09954196214675903, 0.10015695542097092, 0.09998030215501785, 0.10055569559335709, 0.1000969335436821, 0.10119472444057465, 0.09963137656450272, 0.10057053714990616, 0.10076525807380676, 0.1004161462187767, 0.10084996372461319, 0.10082674026489258, 0.10187660902738571, 0.10067082941532135, 0.10113576799631119, 0.10153460502624512, 0.10140953958034515, 0.10207550972700119, 0.10229378938674927, 0.10224680602550507, 0.10138828307390213, 0.10255946218967438, 0.10342524945735931, 0.10186712443828583, 0.10292845964431763, 0.10301462560892105, 0.10319531708955765, 0.10258326679468155, 0.10410124063491821, 0.10346455127000809, 0.1034168004989624, 0.1042090579867363, 0.10373231023550034, 0.10357522964477539, 0.10419563949108124, 0.1045362576842308, 0.10446950048208237]\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for i, (xx, yy) in enumerate(loader):\n",
    "        y_pred = net(xx)\n",
    "        loss = loss_fn(y_pred, yy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_losses.append(running_loss / i)\n",
    "    y_pred = net(X_test)\n",
    "    test_loss = loss_fn(y_pred, Y_test)\n",
    "    test_losses.append(test_loss.item())\n",
    "print('train_losses   :', train_losses)\n",
    "print('--------------------------------------------')\n",
    "print('test_losses   :', test_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# 확률 0.5로 랜덤으로 변수의 차원을\n",
    "# 버리는 Dropout을 각 층에 추가\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(64, k),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(k, k),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(k, k),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(k, k),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(k, 10)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    # 신경망을 훈련 모드로 설정\n",
    "    net.train()\n",
    "    for i, (xx, yy) in enumerate(loader):\n",
    "        y_pred = net(xx)\n",
    "        loss = loss_fn(y_pred, yy)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_losses.append(running_loss / i)\n",
    "    # 신경망을 평가 모드로 설정하고\n",
    "    # 검증 데이터의 손실 함수를 계산\n",
    "    net.eval()\n",
    "    y_pred = net(X_test)\n",
    "    test_loss = loss_fn(y_pred, Y_test)\n",
    "    test_losses.append(test_loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features,\n",
    "                  out_features,\n",
    "                  bias=True, p=0.5):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features,\n",
    "                                out_features,\n",
    "                                bias)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "    CustomLinear(64, 200),\n",
    "    CustomLinear(200, 200),\n",
    "    CustomLinear(200, 200),\n",
    "    nn.Linear(200, 10)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, in_features,\n",
    "                  out_features):\n",
    "        super().__init__()\n",
    "        self.ln1 = CustomLinear(in_features, 200)\n",
    "        self.ln2 = CustomLinear(200, 200)\n",
    "        self.ln3 = CustomLinear(200, 200)\n",
    "        self.ln4 = CustomLinear(200, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.ln3(x)\n",
    "        x = self.ln4(x)\n",
    "        return x\n",
    "\n",
    "mlp = MyMLP(64, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}